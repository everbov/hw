{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.svm\n",
    "import sklearn.model_selection\n",
    "import re as re\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive.csv',sep=';',error_bad_lines=False, header=None).iloc[:,[3,4]]\n",
    "negative = pd.read_csv('negative.csv',sep=';',error_bad_lines=False, header=None).iloc[:,[3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114911, 2) (111923, 2)\n"
     ]
    }
   ],
   "source": [
    "# проверка на сбалансированность\n",
    "print(positive.shape,negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## объединим позитивные и негативные вместе\n",
    "inp=pd.concat([positive,negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## удалим упоминания других участников беседы\n",
    "inp[3] = inp[3].map(lambda x:re.sub(r'[R][T]\\s+[@]\\w+[:]\\s*|[@]\\w+\\s+','', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>хоть я и школота, но поверь, у нас то же самое...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ну ты идиотка) я испугалась за тебя!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Кто то в углу сидит и погибает от голода, а м...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Вот что значит страшилка :D\\nНо блин,посмотрев...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   3  4\n",
       "0  хоть я и школота, но поверь, у нас то же самое...  1\n",
       "1  Да, все-таки он немного похож на него. Но мой ...  1\n",
       "2             Ну ты идиотка) я испугалась за тебя!!!  1\n",
       "3  \"Кто то в углу сидит и погибает от голода, а м...  1\n",
       "4  Вот что значит страшилка :D\\nНо блин,посмотрев...  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(inp[3], inp[4], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204150,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22684,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = sklearn.model_selection.GridSearchCV(\n",
    "    sklearn.pipeline.Pipeline([('vectorize', None),('classifier', sklearn.svm.LinearSVC())]),\n",
    "    param_grid={\n",
    "        'vectorize':[\n",
    "            ##  default max_df=1.0 min_df=1.0\n",
    "            \n",
    "            sklearn.feature_extraction.text.CountVectorizer(),\n",
    "            \n",
    "            ## min_df=5 слово должно встречаться минимум в 5 докумнтах\n",
    "            ## max_df=0.9 уберем слова которые встречаются более чем в 90 процентах докуметов\n",
    "            \n",
    "            sklearn.feature_extraction.text.CountVectorizer(max_df=0.9, min_df=5),\n",
    "            \n",
    "            sklearn.feature_extraction.text.TfidfVectorizer(),\n",
    "            \n",
    "             ## min_df=5 слово должно встречаться минимум в 5 докумнтах\n",
    "            ## max_df=0.9 уберем слова которые встречаются более чем в 90 процентах докуметов\n",
    "            sklearn.feature_extraction.text.TfidfVectorizer(max_df=0.9, min_df=5),\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vectorize', None), ('classifier', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'vectorize': [CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "   ...\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 47.4159685 ,  40.81866908,   6.57856464,   6.37035108]),\n",
       " 'mean_score_time': array([ 1.66429766,  1.4934477 ,  1.56945491,  1.51709692]),\n",
       " 'mean_test_score': array([ 0.72557923,  0.72765124,  0.74047514,  0.73260838]),\n",
       " 'mean_train_score': array([ 0.96113642,  0.82197404,  0.93733039,  0.81710997]),\n",
       " 'param_vectorize': masked_array(data = [ CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)\n",
       "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.9, max_features=None, min_df=5,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None)\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.9, max_features=None, min_df=5,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'vectorize': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)},\n",
       "  {'vectorize': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.9, max_features=None, min_df=5,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)},\n",
       "  {'vectorize': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)},\n",
       "  {'vectorize': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=0.9, max_features=None, min_df=5,\n",
       "           ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)}),\n",
       " 'rank_test_score': array([4, 3, 1, 2]),\n",
       " 'split0_test_score': array([ 0.72681852,  0.72902278,  0.74104335,  0.73257899]),\n",
       " 'split0_train_score': array([ 0.96141073,  0.82215283,  0.93757531,  0.81739897]),\n",
       " 'split1_test_score': array([ 0.72721528,  0.73050698,  0.74226304,  0.73544453]),\n",
       " 'split1_train_score': array([ 0.96118295,  0.82083027,  0.93735489,  0.81623071]),\n",
       " 'split2_test_score': array([ 0.72270389,  0.72342395,  0.73811903,  0.72980162]),\n",
       " 'split2_train_score': array([ 0.96081558,  0.82293902,  0.93706098,  0.81770022]),\n",
       " 'std_fit_time': array([ 1.24741545,  0.37307221,  0.05815425,  0.05061348]),\n",
       " 'std_score_time': array([ 0.14131454,  0.01298792,  0.00988224,  0.01218818]),\n",
       " 'std_test_score': array([ 0.00203961,  0.00304994,  0.00173884,  0.0023038 ]),\n",
       " 'std_train_score': array([ 0.00024519,  0.00087012,  0.00021069,  0.00063377])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predict=grid_search.best_estimator_.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.75      0.75      0.75     11198\n",
      "          1       0.75      0.75      0.75     11486\n",
      "\n",
      "avg / total       0.75      0.75      0.75     22684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8363, 2835],\n",
       "       [2832, 8654]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test,predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
